# Sakana Desktop Assistant Configuration

# LLM Settings
llm_provider: "local"  # Options: local, openai, anthropic
model_name: "llama-3.2-3b"  # Model to use
api_key: null  # Set via environment variable or here
temperature: 0.7
max_tokens: 2048

# Memory Settings
max_short_term_memory: 100
max_long_term_memory: 10000

# Learning Settings
learning_rate: 0.01
evolution_generations: 10
population_size: 20
mutation_rate: 0.1

# Security Settings
sandbox_enabled: true
max_execution_time: 30  # seconds
allowed_operations:
  - read_file
  - write_file
  - execute_code
  - search_web
  - manage_tasks

# UI Settings
enable_gui: false  # GUI not yet implemented
enable_voice: false
theme: "dark"